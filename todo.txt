next steps:
    double check new default gates in gate_sequencer
    finish debugging nvm 0.0 syngen
    positive neural activity with sigmoid, relu, or hard relu fake inverse
    no copying
    check Z that randomly interpolates X,Y
    distinct excitatory/inhibitory populations
    outgoing connections are all to excitatory or all to inhibitory
    axons can excite either excitatory or inhibitory neurons in other regions
    axons can only inhibit within region
    expose gating, user-oriented library for building instructions

dl new neural computation pubs

empirical experiments:
    how size of N_HGATES (and N_LAYER?) affects stability in long loops
    noise injection

why does loop but not look label show up in xlabels?

synapto aas
if W_RAM doesn't have token->hidden, jmp only needs to copy to hidden, not token.  so, label doesn't even have to be paired with hidden, it can *be* the hidden pattern.
memory read/write and n-back program
call/return instructions
look into memh larger than n_layer (need non-square W's mapping up and down)
consider no g->g, only hg -> g
post-flash tokens
more robustness to noise?

first lram infinity run!!! needed 2048 n_layer (will barely fit on outpost) and worked with fewer stability iterations.  and rare; same code with different randomness usually fails.  doesn't always fail at same place.  at least one run had a successful iteration before failing.

problems with local learning:
    even with 0 sign changes in one-step test, residual errors build up.  was able to recover desired behavior with "temp memory test" branch inside tick(). (one of the last commits on monday feb 12)

rank problems with to,from:
    tried a little 2x2 on paper: to1, to2, fr1, fr2.
    [h  h  h  h
     t1 t1 t2 t2   * [-1 1 1 -1].T = [0 0 0].T
     f1 f2 f1 f2]
    so actually low rank.
lork->lork (matrank_scratch.py) beats problem for load, but set+load simultaneous has problem.
flash_rom resolves issue with lork->lork fix applied to *all* transits at once, not one instruction at a time.

flash_rom had issue with simultaneous set+load, but for first time, bigger hidden layer actually did fix.  work out mathematical requirements on layer sizes.

still need to reconcile lork-lork solution with nearly fixed points near +/- pad

lp and learn_seqs one step further
    ensure all openings *and blockages* for every k-1 to k to k+1
    for v^x and v^y, if just any one i^delta in v^y reverts to v^x, the resulting corner must be blocked by the asymptotes that become openings at v^y

show weight matrices in viz
handle activity gating outside the transition function?

competition idea: delta w_ij proportional to sum_t x_i^t W_ij^t x_j^t over recent t.  If this number is large, x_j is in agreement with others about the affect on x_i.  If it is small, x_j is in disagreement, competing.  So "more important", i.e. it's absence could be missed in the borderline cases.  however, don't make *too* large, or else x_j could overrule others when it shouldn't.
systematic comparison of learn rules, local and global
