next steps:
    ref/drf
    ipop
    instruction pitfalls:
        unlinked tokens in nvm_linker
        missing internal connectivity in nvm_net
        uninitialized internal layers in nvm_net
    raise more during assembly
        sub to label that doesn't exist
    deal with unlearning better
        current stack trick won't work for mf->device if past association isn't active
        unlearning only needs input layer (use W*input in place of output)
        biorealism: read schiess2016biobackprop and refs (backprop within dendritic tree but not across synapses)
    auto-associative devices for better token stability
        and remove decay gate, instead letting auto-update stay on usually
    overall diffcounts
    make layer/activator-specific:
        omega and beta for gain
        layer shape, size, activator
    generalize learning rules to any number of X/Y layers
    more biorealistic gating
        multiplicative gating is possible: mehaffy2005mult, salinas2001modulation, shepherd1985spines, koch1983nonlin
        dendritic vs somatic inhibition is multiplicative vs additive
        although, to be fair, dendritic inhibition was apical (right before entering soma, not at subsets of dendritic tree leaves)
        modulation is "divisive": higher inhibition *divides* the firing rate.  high gate output should = high inhibition.
        also intertwined with dendritic backpropagation.
    cts gate output
    wm decay/erase: antilearning (subtract hebbian contribution)
    dynamics move through htn finite state machine?
    talk to greg about:
        only one input module per layer?
        in input callback I need to explicitly set 0 after first tick
        for tc I needed to overcompensate to cancel out previous activity
    smarter linking (only what's necessary)
    namespaces for clobbered human-readable labels
    different coding semantics/errors when pattern provided and token already used?
    check Z that randomly interpolates X,Y and multiple intermediate Z steps
    distinct excitatory/inhibitory populations
    outgoing connections are all to excitatory or all to inhibitory
    axons can excite either excitatory or inhibitory neurons in other regions
    axons can only inhibit within region
    expose gating, user-oriented library for building instructions
    base gate sequencer?
    study more carefully:
        increasing hidden gate layer improves stability?
        smaller pad improves stability?

empirical experiments:
    how size of N_HGATES (and N_LAYER?) affects stability in long loops
    noise injection

memory read/write and n-back program
call/return instructions
more robustness to noise?

rank problems with to,from:
    tried a little 2x2 on paper: to1, to2, fr1, fr2.
    [h  h  h  h
     t1 t1 t2 t2   * [-1 1 1 -1].T = [0 0 0].T
     f1 f2 f1 f2]
    so actually low rank.
lork->lork (matrank_scratch.py) beats problem for load, but set+load simultaneous has problem.

competition idea: delta w_ij proportional to sum_t x_i^t W_ij^t x_j^t over recent t.  If this number is large, x_j is in agreement with others about the affect on x_i.  If it is small, x_j is in disagreement, competing.  So "more important", i.e. it's absence could be missed in the borderline cases.  however, don't make *too* large, or else x_j could overrule others when it shouldn't.
systematic comparison of learn rules, local and global
