EXPERIMENTAL METHODS FOR MEMORY STUDY

Nutshell:
Key-value memory with sequenced keys
Nodes can have continuous values, but keys are binary, and value read-outs are also binary by passing through sign (+/-1) or round (0/1).

Abstraction layer ops:
v = read(k)
write(k,v)
k = first()
k' = next(k)
# k = new()

* "new" key: one which has not already been written with a value.  next(new) should also be new, and so on
* or, no "new" construct, memory management complete responsibility of programmer.  just a first() that returns the very first address.

Operating modes: "Active" mode performs an operation.  "Passive" mode recalls and strengthens previously stored memories

KVN Test setup:
Linked list construction and retrieval
Random number of lists
Random tails for each list (including "NIL" equivalent as well as other "cons cell" equivalents previously constructed
Random passive time between each list
During construction, save every key-value pair written, and the full key sequence from every next() call
During evaluation, test next() on every consecutive key in the sequence, and read() on every key-value pair written
As sanity check, also traverse each list and make sure they contain expected contents

linked list setup doesn't test overwriting previous memories...
follow-up tests that manipulate the linked lists written.  E.g., reversing one or rotating one.
also one setup where the linked lists are not random, but a causal relation, and the follow-up manipulations are serving queries 

KKN experiment:
Look into predetermined key sequences that aren't learned
empirical study of optimal capacity
does optimal sequence length trade off performance of key-value? can't really evaluate in isolation

Memory models:

GALIS: like cue/response, but keys are cues (so cues are sequenced) and values are responses.
v = read(k): clamp k, v is fixed point?
write(k,v): learn fixed v with clamped k
strengthen(k,v): learn fixed v with clamped k (maybe smaller learning rate)
k' = next(k): unclamp k and iterate
k = new(): learn k_last -> random new k? iterate dynamics from k_last without learning?

GALIS for key-key only (next and new), hetero-associative for key-value

Heteroassociative key-value, "toggles" for write and/or strengthen:
CHL vs back propogation
different learning rates
FF vs recurrent connections
vanilla (error mimimization)
error minimization with dropout
weight minimization, error constraint
weighted weights ("competition" terms)

FULL NVM

show weight matrices in viz
handle activity gating outside the transition function?
hidden layer to avoid workarounds

realnet:
Contrastive hebbian learning in fully connected recurrent hopfield with key, value, and hidden layer, first two clamped.  Test hypothesis that weight dropout prevents overwrite/interference.  Look for connection with # distinct output patterns that need to be stored (if small, e.g. true/false/nil/_)

Look for way to leverage history, time locality not limited to one.

seq: Output should become fixed, previously stored become just barely not fixed to avoid overwrite

competition idea: delta w_ij proportional to sum_t x_i^t W_ij^t x_j^t over recent t.  If this number is large, x_j is in agreement with others about the affect on x_i.  If it is small, x_j is in disagreement, competing.  So "more important", i.e. it's absence could be missed in the borderline cases
