\documentclass[pdftex,12pt,letterpaper]{article}

\usepackage[letterpaper,margin=1.0in]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx}

\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\eqdef}{\overset{\text{def}}{=}}
\newcommand{\sign}{\text{\upshape{sign}}}
\newcommand{\suchthat}{\,|\,}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}

\begin{document}
\title{Flashing Dynamics}
\date{}
\maketitle

These notes explain a (non-local) method for constructing weight matrices that produce desired activation dynamics (thought of as analagous to one-time flashing of read-only memory).  The method might justifiably be used as a shortcut to emulate the result of evolutionary/developmental processes in the brain, but is probably not a biologically plausible learning rule.

\section{Basic model, basic trick}

The basic neurodynamical model is $v \mapsto \sigma(Wv)$, where $v\in\mathbb{R}^N$ is an $N$-dimensional vector describing the state of the neural system, $W\in\mathbb{R}^{N\times N}$ is an $N\times N$ matrix of connection weights, and $\sigma$ is an activation function applied element-wise.

Suppose we have a set of $P$ paired patterns $x^{(p)}\mapsto y^{(p)}\in\mathbb{R}^N$.  If the network is in state $x^{(p)}$ at one time step, we want it to be in state $y^{(p)}$ at the next time step.  For example, suppose we want the network to have a limit cycle $v^{(0)}, v^{(1)}, ..., v^{(P-1)}, v^{(P)} = v^{(0)}$.  Then we would write $x^{(p)} = v^{(p)}$ and $y^{(p)} = v^{(p+1 \mod P)}$, for $p\in\{0,1,...,P-1\}$.

The pairs $x^{(p)}$ and $y^{(p)}$ can be arranged as columns of two $N\times P$ matrices $X$ and $Y$, respectively.  With that notation, we seek a $W$ that solves $Y = \sigma(WX)$, or equivalently:
\begin{align}
\sigma^{-1}(Y) = WX\label{eq:cons}
\end{align}

When $X$ and $Y$ are given, this is a linear equation in $W$, so it can be solved with one-liners in Matlab or Python (using numpy).  However, existence of a solution generally requires that $P\leq N$, and that $X$ have full rank $P$.  Relatedly, $X\mapsto Y$ must be deterministic: if two columns of $X$ have the same pattern, the corresponding columns of $Y$ must also have the same pattern.  The trick also requires that $\sigma$ is invertible, so any of tanh, logistic, or softplus should work.  Technically relu$(v) = \max(v,0)$ won't work, but if we set the ``inverse'' of relu$^{-1}(0)$ to something like $-1$ it's possible that might give reasonable results in practice.

\section{Wiggle room}

If $P=N$, the solution for $W$ is unique.  If $P<N$, there is more ``wiggle room:'' any $N\times N$, rank ($N-P$) matrix $M$, with rows in the null space of $X$ (i.e., $MX = \mathbbold{0}^{N\times P}$), can be added to $W$ without breaking Eq.\@ (\ref{eq:cons}).  The right choice of $M$ can be used to improve $W$.\footnote{Use $\mathbbold{0}$ and $\mathbbold{1}$ to denote matrices containing all $0$'s or all $1$'s, with the sizes indicated in superscript.}

For example, once we have solved for an initial $W$, suppose we want a perturbation $M+W$ that is as close as possible to the identity matrix $I$.  To compute $M$, we can first compute an $(N-P)\times N$ basis $B$ for the null space of $X$ with an SVD invocation.  Then I believe we can use a least-squares linear solver on $AB = I-W$ to get the $N\times (N-P)$ coefficient matrix $A$, and set $M = AB$.

As another example, a linear programming solver can be used to impose additional inequality constraints on $W$ while solving Eq.\@ (\ref{eq:cons}).  Since linear programming is generally formulated in terms of vector rather than matrix solutions, $W$ can be solved one row at a time.  Actually, this could be one way to make all outgoing and/or ingoing connections for any particular unit have the same sign, if the corresponding linear program turns out to be feasible.

\textbf{\textit{Creative use of the wiggle room could be a potential strategy to improve stability of the target dynamics.}}

\section{Relation to local learning rules}

For random $X, Y \in [-1,1]^{N\times P}$, with $P \leq N$, there is the nice property that $\mathbb{E}[X^TX/N] = I$, where $\mathbb{E}[\cdot]$ is expected value and $I$ is the $P\times P$ identity matrix.  This means that the weight matrix $\sigma^{-1}(Y)X^T/N$ will roughly solve Eq.\@ (\ref{eq:cons}) for $W$, with $\sigma = \tanh$.  This is more or less temporally asymmetric Hebbian learning.

If instead we choose random $X, Y\in [0,1]^{N\times P}$ (non-negative neural activity), we need a change of variables to leverage the nice property: $\mathbb{E}[(2X-1)^T(2X-1)/N] = I$.  With some algebra and the introduction of a bias term I believe one can derive an analogous rule for $\sigma(WX + b) = Y$, with $\sigma =$ logistic, by setting
\begin{align}
W &= 2\sigma^{-1}(Y)(2X^T - \mathbbold{1}^{P\times N})/N \\
b &=  \sigma^{-1}(Y)(\mathbbold{1}^{P\times N} - X^T)\mathbbold{1}^{N\times 1}/N
\end{align}
although I haven't tested this yet.

\section{Intermediate time steps}

If $X$ has rank lower than $P$, there may be no $W$ solving Eq.\@ (\ref{eq:cons}); particularly, if $\sigma^{-1}(Y)$ is full rank but $X$ isn't.  On the other hand, there is the possibility of mapping $X$ onto another matrix with the same low rank.  What's more, if we pass that low rank matrix through $\sigma$, perhaps it becomes full rank since entries are scaled non-linearly.  Based on this idea, there is hope of going from $X$ to $Y$ if we allow an intermediate time step between them.  This should be related to the well known result that multilayer perceptrons are universal function approximators but single-layer perceptrons are not.

More formally, suppose $X$ is size $N\times P$ but has rank $Q < P$.  We seek a weight matrix $W$ as well as intermediate pattern matrix $Z$ satisfying both $Y = \sigma(WZ)$ and $Z = \sigma(WX)$.  To generate $Z$, first let $B$ be a full rank, $Q\times P$ basis for the row space of $X$.   For example, let $B$ be the transposed right singular vectors of $X$.  Then sample a random $N\times Q$ matrix $R$, and set $Z = \sigma(RB)$.  Finally, solve the following augmented Eq.\@ (\ref{eq:cons}) for $W$:

\begin{align}
W[X, Z] = \sigma^{-1}([Z, Y]),
\end{align}
where $[\cdot,\cdot]$ denotes horizontal matrix concatenation.  The resulting $W$ will produce the dynamics $X\mapsto Z\mapsto Y$.

Since $RB = \sigma^{-1}(Z)$ is the same rank as $X$, we can expect existence of a solution to $RB = WX$.  Moreover, application of $\sigma$ generally seems to transform a low rank matrix into a full rank matrix, so $Z$ will generally be full rank and we can also expect existence of a solution to $\sigma^{-1}(Y) = WZ$.  I'm not sure exactly what a rigorous proof would look like but so far it has worked in practice.

\textbf{\textit{Different sampling distributions for $R$ could be another source of wiggle room for improving stability.}}

\section{Application to NVM}

Here's how this approach was used in the initial version of the NVM.  Let $a$, $h$, and $g$ be the input, hidden, and output layers of a gating region.  In the presense of a particular input $a$, we'd like to produce a sequence of gate activity $g^{(0)}, g^{(1)}, ...$.  This sequence could very well require non-determinism depending on the goal of the gating operations (e.g., open gate 1, open gate 2, open gate 1 again, open gate 3).  The role of the hidden layer is to counteract that and have a net deterministic effect.  For example:
\begin{align}
\left[\begin{array}{c}g^{(0)} \\ h^{(0)} \\ a^{(0)}\end{array}\right]\mapsto
\left[\begin{array}{c}g^{(1)} \\ h^{(1)} \\ a^{(0)}\end{array}\right]\mapsto
\left[\begin{array}{c}g^{(0)} \\ h^{(2)} \\ a^{(0)}\end{array}\right]\mapsto
\left[\begin{array}{c}g^{(2)} \\ h^{(3)} \\ a^{(0)}\end{array}\right]\mapsto
\left[\begin{array}{c}g^{(0)} \\ h^{(0)} \\ a^{(0)}\end{array}\right]
\end{align}
This sequence is deterministic since the hidden pattern is different in every step - until the last step, where a limit cycle is formed that would bring the gate layer back to its initial state so it can process a new instruction.

Then there are many such sequences with different input patterns $a$, representing the desired gating behavior in the presence of different opcodes and operands.  These sequences are all concatenated horizontally to form the final $X$ and $Y$ matrices before solving for $W$.  Here another pitfall is that certain structure in the input layer can produce an $X$ that is low rank.  As a toy example, in the \texttt{mov to from} instruction, where \texttt{to} and \texttt{from} are destination and source layers, there was a gating sequence for every possible pair of layers.  This results in matrices that are larger versions of
\begin{align}
\left[\begin{array}{cccc}g^{(0)}&g^{(0)}&g^{(0)}&g^{(0)} \\ h^{(0)}&h^{(0)}&h^{(0)}&h^{(0)} \\ a^{\texttt{to}_1}&a^{\texttt{to}_1}&a^{\texttt{to}_2}&a^{\texttt{to}_2} \\ a^{\texttt{from}_1}&a^{\texttt{from}_2}&a^{\texttt{from}_1}&a^{\texttt{from}_2}\end{array}\right]
\end{align}
Multiplication by $[1, -1, -1, 1]^T$ on the right shows that this matrix is not full rank.  Therefore intermediate steps were needed.  What's more, those intermediate steps required all gates to be off, or else each intended gate opening would last an extra time step and could derail the target dynamics (e.g., leave memory dynamics open and the program tokens would be visited twice as fast as they should be).

One last wrinkle is that the input layer doesn't receive any direct connections from the output or hidden layers (although it will be gradually influenced by larger loops outside the gate region).  So in fact we are learning a weight matrix $W$ of size $(N^{(g)}+N^{(h)})\times(N^{(g)}+N^{(h)}+N^{(a)})$ where each $N$ is the size of the layer in its respective superscript.  Moreover, $a$ is actually a concatenation of four layers: the opcode and three operands.  But at any given time, connections from some subset of those four might be gated.  In that case, the gated portions should not influence $W$ during the linear solve, or else $W$ would rely on some biasing input that would not actually be present when its gate is closed during execution.  The way this was handled was by setting the gated-to-be portions of $a$ to zero during the solve.

The net result is:
\begin{itemize}
\item A matrix $X$ with the vertical concatenation of the $g$, $h$, and $a$ sequences
\item An intermediate step matrix $Z$ the same size as $X$, but with all gates closed in the first $N^{(g)}$ rows, all input zero in the last $N^{(a)}$ rows, and the middle $N^{(h)}$ rows randomly generated as described in the previous section
\item A final matrix $Y$ with $N^{(g)}+N^{(h)}$ rows (no rows for input, since they have no direct connections from $g$ and $h$, as described above).  Similarly, the second occurrence of $Z$ in $[X,Z],[Z,Y]$ is also truncated to its first $N^{(g)}+N^{(h)}$ rows.  This results in a correctly sized $W$.
\end{itemize}

These steps are implemented in the \texttt{flash\_rom()} method of \texttt{flash\_rom.py}.

\textit{\textbf{If all else fails, I've seen anecdotally that stability seems to improve when $N^{(h)}$ is increased, but I haven't tested this systematically.}}

\end{document}

