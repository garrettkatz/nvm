\documentclass[pdftex,12pt,letterpaper]{article}

\usepackage[letterpaper,margin=1.0in]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\eqdef}{\overset{\text{def}}{=}}
\newcommand{\sign}{\text{\upshape{sign}}}
\newcommand{\suchthat}{\,|\,}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}

\begin{document}
\title{Flashing Dynamics}
\date{}
\maketitle

These notes explain a (non-local) method for constructing weight matrices that produce desired activation dynamics (thought of as analagous to one-time flashing of read-only memory).  These might justifiably be used to model the result of evolutionary/developmental processes, but probably not learning processes, in the brain.

\section{Basic model, basic trick}

The basic dynamical model is $v \mapsto \sigma(Wv)$, where $v\in\mathbb{R}^N$ is an $N$-dimensional vector describing the state of the neural system, $W\in\mathbb{R}^{N\times N}$ is an $N\times N$ matrix of connection weights, and $\sigma$ is an activation function applied element-wise.

Suppose we have a set of $P$ paired patterns $x^{(p)}\mapsto y^{(p)}\in\mathbb{R}^N$.  If the network is in state $x^{(p)}$ at one time step, we want it to be in state $y^{(p)}$ at the next time step.  For example, suppose we want the network to have a limit cycle $v^{(0)}, v^{(1)}, ..., v^{(P-1)}, v^{(P)} = v^{(0)}$.  Then we would write $x^{(p)} = v^{(p)}$ and $y^{(p)} = v^{(p+1 \mod n)}$, for $p\in\{0,1,...,n-1\}$.

The pairs $x^{(p)}$ and $y^{(p)}$ can be arranged as columns of two $N\times P$ matrices $X$ and $Y$, respectively.  With that notation, we seek a $W$ that solves $Y = \sigma(WX)$, or equivalently:
\begin{align}
\sigma^{-1}(Y) = WX\label{eq:cons}
\end{align}

When $X$ and $Y$ are given, this is a linear equation in $W$, so it can be solved with one-liners in Matlab or Python (using numpy).  However, existence of a solution generally requires that $P\leq N$, and that $X$ have full rank $P$.  It also requires that $\sigma$ is invertible, so any of tanh, logistic, or softplus should work.  Technically relu$(v) = \max(v,0)$ won't work, but if we set the ``inverse'' of relu$^{-1}(0)$ to something like $-1$ it's possible that might give reasonable results in practice.

\section{Wiggle room}

If $P=N$, the solution for $W$ is unique.  If $P<N$, there is more ``wiggle room:'' any matrix $M$ in the null space of $X$ (i.e., $MX = \mathbf{0}$) can be added to $W$ without breaking (\ref{eq:cons}).  The right choice of $M$ can be used to improve $W$.\footnote{Use $\mathbf{0}$ and $\mathbf{1}$ to denote matrices containing all $0$'s or all $1$'s, sized appropriately for any given equation.}

For example, once we have solved for an initial $W$, suppose we want a perturbation $M+W$ that is as close as possible to the identity matrix $I$.  To compute $M$, we can first compute a basis $B$ for the null space of $X$ with an SVD invocation.  Then we can use a least-squares linear solver on $AB = I-W$ to get the coefficient matrix $A$, and set $M = AB$.

As another example, a linear programming solver can be used to impose additional inequality constraints on $W$ while solving (\ref{eq:cons}).  Since linear programming is generally formulated in terms of vector rather than matrix solutions, $W$ can be solved one row at a time.  Actually, this could be one way to make all outgoing and/or ingoing connections for any particular unit have the same sign, if the resulting linear program turns out to be feasible.

Creative use of the wiggle room could be a potential strategy to improve stability of the target dynamics.

\section{Relation to local learning rules}

For random $x^{(p)} \in [-1,1]^N$, with $P \leq N$, there is the nice property that $\mathbb{E}[X^TX/N] = I$, where $\mathbb{E}[\cdot]$ is expected value and $I$ is the $P\times P$ identity matrix.  This means that the weight matrix $\sigma^{(-1)}(Y)X^T/N$ will roughly solve (\ref{eq:cons}).  This is more or less temporally asymmetric Hebbian learning.

If instead we choose random $x^{(p)}\in [0,1]^N$ (non-negative neural activity), we need a change of variables to leverage the nice property: $\mathbb{E}[(2X+1)^T(2X+1)/N] = I$.  With some algebra and the introduction of a bias term there should be an analagous rule for learning $W$ and $\theta$ satisfying $Y = \sigma(WX + \theta)$.

\section{Intermediate time steps}

If $X$ has rank lower than $P$, there may be no $W$ solving (\ref{eq:cons})

\section{Application to NVM}

\end{document}

