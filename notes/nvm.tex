\documentclass[pdftex,12pt,letterpaper]{article}

\usepackage[letterpaper,margin=1.0in]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\eqdef}{\overset{\text{def}}{=}}
\newcommand{\sign}{\text{\upshape{sign}}}
\newcommand{\suchthat}{\,|\,}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}

\begin{document}
\title{NVM update}
\date{}
\maketitle

\section{NVM instruction set}

Below is a target program that we would like a neural virtual machine (NVM) to execute (Table \ref{tbl:aas}).  This program carries out the affective prosaccade task in Reinhard 2017 (Fig. \ref{fig:aas_task}).  With a few additional lines it could also carry out an antisaccade.  In this program, the NVM maintains a central fixation point in the frontal eye fields (FEF) until the temporal cortex (TC) successfully determines a gaze direction in an affective face image.  Once the gaze direction is determined, the NVM changes the fixation point in the FEF to match the gaze direction.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../resources/aas_task.png}
\caption{The affective saccade task condition.}
\label{fig:aas_task}
\end{figure}

The program in Table \ref{tbl:aas} is based on the domain-independent instruction set in Table \ref{tbl:inst}.  These instructions were designed to be generic and domain-independent so that they can be reused without modification for other cognitive tasks like N-back and DSST, although those tasks may also require other instructions in addition to those already implemented.

The NVM instruction set is very similar to a typical assembly language instruction set.  Each instruction consists of a label, an opcode, and several operands.  The label is an optional control flow identifier - loops and conditional branches use labels to indicate where program execution should continue next.  The opcodes indicate what instruction to do next, and the operands indicate what to do it to.  Some operands are values, like \texttt{left}, which represents a leftward gaze direction.  Others are brain regions like \texttt{FEF}, or more general-purpose ``registers'' like \texttt{reg1} that could represent layers of neurons in, say, dlPFC.

\begin{table}[H]
\begin{tabularx}{\textwidth}{lllllX}
Label & Opcode & Op 1 & Op 2 & Op 3 & Comment \\
\hline
    & \texttt{set} & \texttt{reg2} & \texttt{right} & & Store right gaze for comparison with TC \\
    & \texttt{set} & \texttt{reg3} & \texttt{left} & & Store left gaze for comparison with TC \\
 & \texttt{set} & \texttt{FEF} & \texttt{center} & & Fixate on center \\
\texttt{loop} & \texttt{cmp} & \texttt{reg1} & \texttt{TC} & \texttt{reg2} & Check if TC detects rightward gaze \\
    & \texttt{jmp} & \texttt{reg1} & \texttt{look} & & If so, skip to saccade step \\
    & \texttt{cmp} & \texttt{reg1} & \texttt{TC} & \texttt{reg3} & Check if TC detects leftward gaze \\
    & \texttt{jmp} & \texttt{reg1} & \texttt{look} & & If so, skip to saccade step \\
    & \texttt{set} & \texttt{reg1} & \texttt{true} & & If here, gaze not known yet, prepare jump \\
    & \texttt{jmp} & \texttt{reg1} & \texttt{loop} & & Check for gaze again \\
\texttt{look} & \texttt{mov} & \texttt{FEF} & \texttt{TC} & & TC detected gaze, overwrite FEF with gaze direction \\
    %% & RET & NULL & NULL & NULL & Successful saccade, terminate program \\
\end{tabularx}
\caption{Program implementing an antisaccade trial}
\label{tbl:aas}
\end{table}

The program in Table \ref{tbl:aas} operates as follows.  First, two registers \texttt{reg1} and \texttt{reg2} are set to have two values, \texttt{right} and \texttt{left}, which represent rightward and leftward gazes respectively.\footnote{This is ``boilerplate'' that is needed because the \texttt{cmp} operands are register names, not values.  It could be obviated with a more expressive instruction set.}  Next, the FEF is assigned the \texttt{center} value to enforce a central fixation point.  It is presumed that in parallel with program execution, separate sub-cortical regions (e.g. Greg's superior colliculus model) will control the eyes based on FEF activity, resulting in fixations and saccades.  At this point, a loop begins (labeled with \texttt{loop}).  In each iteration of this loop, the current value of TC is compared first with \texttt{right} (stored in \texttt{reg2}) and then \texttt{left} (stored in \texttt{reg3}).  The results of the comparison are computed in \texttt{reg1}.  It is presumed that in parallel with this loop, separate visual processing areas are analyzing the face image to detect the gaze direction.\footnote{It may be unrealistic to suppose a repesentation can be formed in TC without the aid of small saccades to facial features that are not quite at the central fixation point.  In that case we might suppose that the FEF signal keeps saccades close but allows for some small movements driven by bottom up signals in other brain regions.}  Eventually the gaze will be determined and represented in TC.  As soon as TC is found to contain one of the two gaze directions, program execution jumps out of the loop to the \texttt{look} label.  It then copies the gaze direction from TC to FEF, to trigger saccadic activity in sub-cortical regions.  Until TC contains one of the two directions, the program will keep looping by jumping back to the \texttt{loop} label and trying again.

\begin{table}[H]
\begin{tabularx}{\textwidth}{llllX}
Opcode & Operand 1 & Operand 2 & Operand 3 & Description \\
\hline
\texttt{set} & \textit{dst} & \textit{value} & & Copy \textit{value} into layer \textit{dst} \\
\texttt{mov} & \textit{dst} & \textit{src} & & Copy the contents of layer \textit{src} to layer \textit{dst} \\
\texttt{cmp} & \textit{dst} & \textit{srcA} & \textit{srcB} & Check if \textit{srcA} and \textit{srcB} contain the same pattern and put the result in \textit{dst} \\
\texttt{jmp} & \textit{layer} & \textit{label} & & If \textit{layer} contains \texttt{true}, jump execution to the instruction labeled \textit{label} \\
%% \texttt{nand} & & & & TODO: NAND circuit (sufficient for expressing any boolean function)\\
%% \texttt{head} & & & & TODO: \\
%% \texttt{tail} & & & & TODO: \\
%% \texttt{cons} & & & & TODO: \\
%% \texttt{call} & & & & TODO: \\
%% \texttt{rtrn} & & & & TODO: \\
\end{tabularx}
\caption{Initial NVM instruction set}
\label{tbl:inst}
\end{table}

\section{NVM architecture}

This initial instruction set has been implemented in a purely neurocomputational NVM architecture, shown in Fig.\@ \ref{fig:arch}.  Registers are implemented as layers of neural units.\footnote{The term ``layer'' here just means a group of neurons; it is not meant to suggest a feed-forward architecture.  Every layer has connections to itself and almost every other layer.}  Every ``token'' (i.e., values, labels, and register names) is represented as a neural activity pattern that can occur in any of those layers.  In addition to registers, the program can also interact with ``devices,'' which serve as domain-dependent components that enable IO and interaction between the NVM and its ``environment.''  In the antisaccade domain they include FEF for (eventually) producing motor output and TC for (eventually) receiving perceptual input.  In general, any number of devices can be attached to the NVM proper and serve as interfaces between the NVM and other neural or non-neural external systems.  From the NVM's perspective, these are treated as additional layers whose activity can change exogenously.

A program is represented by unwrapping the sequence of its tokens and storing the sequence in a memory layer, using temporally asymmetric Hebbian learning.  Instructions without explicit labels, or fewer than three operands, are padded with a special \texttt{null} pattern so that all instructions occupy five consecutive patterns in the sequence.  For example, the program in Table \ref{tbl:aas} is converted to the sequence:
\[ \text{\texttt{null} \texttt{set} \texttt{reg2} \texttt{right} \texttt{null} \texttt{null} \texttt{set} \texttt{reg3} \texttt{left} \texttt{null} ...} \]
where each token is replaced by its corresponding activity pattern.

Since this sequence contains non-determinism (sometimes \texttt{null} is followed by \texttt{set}, sometimes by \texttt{null}, etc.), it can not actually be stored reliably as is.  To store it successfully with Hebbian learning, the memory region is augmented with a hidden layer, in addition to the ``token'' layer that stores the tokens.  Each pattern in the token sequence is paired with a random hidden pattern during learning.  For example, the first and second occurrences of \texttt{set} will have the same pattern in the token layer, but different patterns in the hidden layer.  This resulting pair of sequences is what gets learned by the memory network.  Hebbian learning is used on the paired sequences to learn recurrent weights within the hidden layer, and associative weights from the hidden layer to the token layer.

Once a program is learned, it is executed as follows.  Every ``clock cycle'', the NVM undergoes several time steps.  At different time steps, different gates in the gate layer are open or closed.  These gates can open flow of activity between regions, dynamics within a region, or temporally asymetric Hebbian learning within a region.  In many cases these gates simply open one-to-one connections that effectively copy activity patterns from one layer to another.  In other cases, they open more complex dynamics in fully connected regions - for example, ungating dynamics in the memory layer, so that it iterates through patterns in a sequence, or ungating learning in the memory layer, so that it stores new items.\footnote{The current instruction set only uses learning in the memory layer to store a program.  But the next version of the instruction set will also include \texttt{read}/\texttt{write} instructions, that use learning to store/retrieve data in addition to instructions in memory.}  Since the necessary sequences of gating patterns may also be non-deterministic, the gate region also contains a hidden layer that functions similarly to the one in the memory layer.  The output gate layer contains the actual gate patterns.  This neural layer contains one neural unit per gate, and the value of that unit determines whether the gate is open or closed.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../resources/nvm_arch.pdf}
\caption{The NVM architecture.  Fully connected layers indicated by block arrows, including full recurrent connections.  One-to-one connections indicated by dashed line arrows.  Gating connections indicated by solid black line curves.}
\label{fig:arch}
\end{figure}

During first time-steps of the clock cycle, the next instruction is loaded into the opcode and operand layers.  This is done by ungating memory dynamics and simultaneously ungating one-to-one connections from the token layer to the opcode layer, then from the token layer to the operand1 layer, etc.  Once the instruction is loaded, memory dynamics are closed.  Then, gates from the opcode and operand layers to the gate layer are opened.  This allows different opcodes to bias the gating dynamics differently, and produce different sequences of gate patterns.  For example, when \texttt{set} is in the opcode layer and \texttt{reg1} is in the operand1 layer, it biases the gate sequence to open one-to-one connections between the operand2 layer and the register 1 layer, thereby overwriting the contents of register 1 with the value in operand 2.  In the last time step, the gate layer returns to the state it was in at the beginning of the clock cycle, so that it will begin loading the next instruction into the opcode and operand layers.

In contrast, when \texttt{jmp} is in the opcode layer and \texttt{true} is in the operand 1 layer, they provide a different bias, and the gate dynamics transition through a different sequence:  this time, they ungate the one-to-one connections between the operand 2 layer (which contains the target label), and the token memory layer.  This effectively overwrites the memory so that it resumes from a different point when the next clock cycle begins.

However, the need for a hidden memory layer introduces a wrinkle in this story and necessitates one additional preprocessing step when encoding the program.  For the sake of example consider the 4th instruction in the saccade program, labeled \texttt{loop}.  Suppose that this token is paired with the hidden pattern \texttt{$h_\texttt{loop}$} when the program is stored as memory.  In order for the penultimate \texttt{jmp} command to work, it must not only load the \texttt{loop} token into the memory token layer, but must also load \texttt{$h_\texttt{loop}$} into the memory hidden layer.  To this end, the unused third operand in the \texttt{jmp} command is treated specially.  Rather than being padded with \texttt{null}, it is filled with \texttt{$h_\texttt{loop}$} immediately before program learning.  Later, when the \texttt{jmp} command is being executed, the gates open not only between operand 2 and token memory, but also between operand 3 and hidden memory.

The requisite sequences of gate dynamics needed for each instruction were ``learned'' using a one-time, non-local construction, based on linear algebraic techniques, to produce the weight matrices $W^{(gates, from)}$, where $from$ is one of $\{$gates, opcode, operand 1, operand 2, operand 3$\}$.  These weights remain fixed during operation of the NVM and can be thought of as resulting from an evolutionary or developmental process.  The weight matrices in the compare region are also fixed and hardwired to compute the \texttt{true} pattern when two input patterns have the same sign in every element.  All other weights are subject to change using local update rules during program execution and learning.  Moreover, the gate dynamics are domain independent - they only need to be constructed once, and then they can run any program implemented using the instruction set.

\section{NVM dynamics}

The activity and weight dynamics in every NVM region work as follows.  By default (all gates closed), each layer will saturate towards a fixed point at the nearest corner of its hypercube state space, using only self-connection weights and bias with constant scalar values $\omega$ and $\beta$.  For example, given a tanh activation function, $\omega > 1$ and $\beta = 0$, and $\omega$ can be thought of as a large gain.  For logistic activation, a non-zero is needed to achieve the same result.  This is useful for sustaining patterns over multiple time steps.  If the ``update'' gate $u_{to,from}$ is open, then signals from layer $from$ to layer $to$ are allowed to propogate, modulated by the connection matrix $W^{(to,from)}$.  This includes recurrent connections when $to = from$, and copies between layers when $W^{(to,from)}$ is a multiple of the identity matrix (i.e., one-to-one connections).  If the ``decay'' gate $d_{layer}$ is open, then the gain is suppressed, and in the absence of input from other layers, the activity in $layer$ will decay towards zero.  This is needed when patterns \emph{shouldn't} be sustained, but should evolve according to inter- or intra- layer dynamics.  When a ``learning'' gate $\ell_{to,from}$ is open, then the connection matrix $W^{(to,from)}$ changes according to a basic temporally asymetric Hebbian learning rule.  These gates are scalars for any given pair $to,from$.  So the total number of each type of gate equals the number of layers squared.

Formally:

\begin{align}
v^{(t+1)}_{to} = \sigma\left((1-\tilde{u}^{(t)}_{to,to})\cdot(1 - \tilde{d}^{(t)}_{to})(\omega v^{(t)}_{to} + \beta) + \sum_{from}\tilde{u}^{(t)}_{to,from}W^{(to,from)}v^{(t)}_{from}\right)
\label{eq:actrule}
\end{align}
where $v^{(t)}_{layer}$ is the vector of activity in layer $layer$ at time $t$, and $\sigma$ is $\tanh$, applied element-wise.\footnote{For greater biological realism, all neural activities should really be positive (negative firing rate isn't physically possible).  However, this can be easily achieved with a change of coordinates that transforms tanh into the logistic sigmoid.  In the next version of the NVM this transformation will be incorporated.}  This rule is repeated with the destination layer $to$ ranging across all layers in the NVM.

Weight dynamics for program learning and memory writes use the following rule:
\begin{align}
\Delta W^{(to,from)} = \tilde{\ell}^{(t)}_{to,from}\sigma^{-1}(v^{(t+1)}_{to})(v^{(t)}_{from})^T/N
\label{eq:ahebb}
\end{align}
where $N$ is the number of neurons in layer $from$.  As an outer product of consecutive activity patterns, this is essentially temporally asymmetric Hebbian learning, with the slight modification that $v^{(t+1)}_i$ is passed through $\sigma^{-1}$ before multiplication with $v^{(t)}_j$ when computing weight $W_{i,j}$.  This counterbalances the contracting effect of $\sigma$, which could otherwise produce dynamics that decay towards zero in the continuous-valued model used here.  When $to$ and $from$ are the same layer, Eq. (\ref{eq:ahebb}) updates its recurrent weights.

In Eqs. (\ref{eq:actrule}) and (\ref{eq:ahebb}), the tilde in $\tilde{u}$ denotes $(1+\sign(u))/2$, and similarly for $\tilde{d}$ and $\tilde{\ell}$,  which maps each gate from $[-1,1]$ to $\{0, 1\}$.  Values of $0$ and $1$ correspond to a gate being closed or open, respectively.

The gate layer activation dynamics also evolve according to Eq.\@ (\ref{eq:actrule}).  In this case, the gate layer $v_{gates}$ is an unraveled vector concatenation of all the $u$, $d$, and $\ell$.  The gate layer never gates itself, so $u_{(gate,gate)}$ and $d_{(gate)}$ are always $1$ in Eq.\@ \ref{eq:actrule} when computing $v_{gates}^{(t+1)}$ specifically.

One other special case is the compare circuit, which, in the spirit of gating mechanisms, also uses second-order connections.  Using $a$, $b$, $h$ and $o$ to denote the input layers, hidden layer, and output layer respectively, the update rule is:
\begin{align}
h_i = \sigma\left(w^ha_ib_i\right) \\
o_i = \sigma\left(w^o\textbf{1}^Th - \theta\right)
\end{align}
where $w^h$, $w^o$, and $\theta$ are all scalars properly chosen so that all $o_i \approx 1$ when all $\sign(a_i) = \sign(b_i)$, and all $o_i \approx -1$ otherwise.  Along the same lines, the activity patterns for special tokens \texttt{true} and \texttt{false} are approximately $\mathbf{1}$ and $-\mathbf{1}$, respectively.

\section{Example runs}

See figures below

\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth]{../resources/lram_left_run1024.eps}
\caption{A raster plot showing execution of the program in Table \ref{tbl:aas} when \texttt{TC} contains \texttt{left}.  The x-axis shows time-steps annotated with the currently executing instruction.  The y-axis shows different NVM layers.  Each row of the image shows one neural unit in one layer over time, where brightness indicates firing rate.}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth]{../resources/lram_right_run1024.eps}
\caption{A raster plot showing execution of the program in Table \ref{tbl:aas} when \texttt{TC} contains \texttt{right}.}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth]{../resources/lram_infinity_run1024.eps}
\caption{A raster plot showing execution of the program in Table \ref{tbl:aas} when \texttt{TC} contains \texttt{null}.}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth]{../resources/lram_infinity_fail1024.eps}
\caption{A raster plot showing failed execution of the program in Table \ref{tbl:aas} when \texttt{TC} contains \texttt{null}.  After a couple loop iterations the activity destabilizes.}
\label{fig:fail}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth]{../resources/lram_infinity_fail1024gates.eps}
\caption{A close-up of the destabilized memory and gate activity from Fig.\@ \ref{fig:fail}.}
\end{sidewaysfigure}


\section{Next up}

\indent * Finish instruction set: call/return, read/write memory

\noindent * N-back (to prove out memory read/writes) and DSST programs (to prove out call/return HTN-like subroutines)

\noindent * Port to synaptogenesis and interface with Greg

\noindent * Run empirical experiments to quantify performance/capacity as function of layer sizes and network damage/noise

\end{document}

